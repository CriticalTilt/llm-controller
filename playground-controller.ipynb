{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "870b2361",
   "metadata": {},
   "source": [
    "# LLMController Playground\n",
    "\n",
    "This notebook demonstrates how to use the LLMController class with Claude and test various LangChain features.\n",
    "\n",
    "## 1. Setup and Installation\n",
    "\n",
    "Install required packages \n",
    "!pip install langchain anthropic openai python-dotenv langchain-community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bce66970",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from typing import Dict, Any, Optional, Union, List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d28b6d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment loaded successfully!\n",
      "Anthropic API Key loaded: Yes\n",
      "OpenAI API Key loaded: Yes\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "print(\"Environment loaded successfully!\")\n",
    "print(f\"Anthropic API Key loaded: {'Yes' if os.getenv('ANTHROPIC_API_KEY') else 'No'}\")\n",
    "print(f\"OpenAI API Key loaded: {'Yes' if os.getenv('OPENAI_API_KEY') else 'No'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e47432a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain version: 0.3.17\n"
     ]
    }
   ],
   "source": [
    "# Check LangChain version\n",
    "try:\n",
    "    import langchain\n",
    "    print(f\"LangChain version: {langchain.__version__}\")\n",
    "except:\n",
    "    print(\"LangChain version not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae5cf343",
   "metadata": {},
   "source": [
    "## 2. Import LLMController Class\n",
    "\n",
    "*Note: Make sure you have the LLMController class from the previous artifact saved as `llm_controller.py`*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3575f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm_controller import LLMController"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4849886d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLMController initialized with Claude!\n",
      "Current model info: {'provider': 'claude', 'model': 'claude-3-sonnet-20240229', 'type': 'ChatAnthropic', 'langchain_structure': 'new'}\n"
     ]
    }
   ],
   "source": [
    "# Initialize the LLMController with Claude\n",
    "llm = LLMController(\n",
    "    llm=\"claude-3-sonnet-20240229\",  # You can change this to claude-3-haiku-20240307 for faster/cheaper\n",
    "    provider=\"claude\",\n",
    "    temperature=0.5 #optional\n",
    ")\n",
    "\n",
    "print(\"LLMController initialized with Claude!\")\n",
    "print(f\"Current model info: {llm.current_model_info}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4da9f620",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Claude's Response:\n",
      "Sure, here's a brief programming joke:\n",
      "\n",
      "Why did the programmer quit his job? Because he didn't get arrays.\n"
     ]
    }
   ],
   "source": [
    "# Test basic invoke functionality\n",
    "response = llm.invoke(\"Hello! Can you tell me a brief joke about programming?\")\n",
    "print(\"Claude's Response:\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a2734f94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Claude's explanation of list comprehensions:\n",
      "Sure, a list comprehension is a concise way to create a new list in Python by applying an expression to each item in an iterable (like a list, tuple, or string). It provides a shorter syntax compared to using a for loop or a lambda function.\n",
      "\n",
      "The basic syntax for a list comprehension is:\n",
      "\n",
      "```python\n",
      "new_list = [expression for item in iterable]\n",
      "```\n",
      "\n",
      "Here's an example that creates a list of squares from a list of numbers:\n",
      "\n",
      "```python\n",
      "numbers = [1, 2, 3, 4, 5]\n",
      "squares = [x**2 for x in numbers]\n",
      "print(squares)  # Output: [1, 4, 9, 16, 25]\n",
      "```\n",
      "\n",
      "You can also add a condition to filter the items in the new list:\n",
      "\n",
      "```python\n",
      "numbers = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "evens = [x for x in numbers if x % 2 == 0]\n",
      "print(evens)  # Output: [2, 4, 6, 8, 10]\n",
      "```\n",
      "\n",
      "List comprehensions can be more compact and readable than using for loops or lambda functions, especially for simple operations. However, for more complex operations, a regular for loop might be more readable.\n",
      "\n",
      "Here's an example of creating a list using a for loop:\n",
      "\n",
      "```python\n",
      "numbers = [1, 2, 3, 4, 5]\n",
      "squares = []\n",
      "for x in numbers:\n",
      "    squares.append(x**2)\n",
      "print(squares)  # Output: [1, 4, 9, 16, 25]\n",
      "```\n",
      "\n",
      "List comprehensions can also be nested to create lists from other lists or iterables. This can be useful for working with matrices or nested data structures.\n",
      "\n",
      "```python\n",
      "matrix = [[1, 2], [3, 4], [5, 6]]\n",
      "flattened = [x for row in matrix for x in row]\n",
      "print(flattened)  # Output: [1, 2, 3, 4, 5, 6]\n",
      "```\n",
      "\n",
      "In summary, list comprehensions provide a concise and expressive way to create new lists by applying an expression to each item in an iterable, with optional filtering conditions. They can make your code more readable and compact, especially for simple operations on lists or iterables.\n"
     ]
    }
   ],
   "source": [
    "# Test with message format (like ChatGPT conversation)\n",
    "\n",
    "# Import message classes if not already available\n",
    "try:\n",
    "    from langchain_core.messages import HumanMessage, AIMessage\n",
    "except ImportError:\n",
    "    from langchain.schema import HumanMessage, AIMessage\n",
    "    \n",
    "messages = [\n",
    "    HumanMessage(content=\"I'm learning Python. Can you explain what a list comprehension is?\"),\n",
    "]\n",
    "\n",
    "response = llm.invoke(messages)\n",
    "print(\"Claude's explanation of list comprehensions:\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6c4ba304",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Using new prompt imports\n",
      "Chain result - Python Decorators:\n",
      "Sure, I'll explain decorators in Python with a simple example.\n",
      "\n",
      "In Python, a decorator is a function that takes another function as an argument, adds some functionality to it, and returns a new function. Decorators are used to modify the behavior of a function without changing its source code. They provide a way to wrap one function with another function to extend its functionality.\n",
      "\n",
      "Here's a simple example to illustrate the concept of decorators:\n",
      "\n",
      "```python\n",
      "def uppercase_decorator(func):\n",
      "    def wrapper():\n",
      "        result = func()\n",
      "        return result.upper()\n",
      "    return wrapper\n",
      "\n",
      "def say_hello():\n",
      "    return \"hello\"\n",
      "\n",
      "# Decorate the say_hello function with uppercase_decorator\n",
      "say_hello = uppercase_decorator(say_hello)\n",
      "\n",
      "print(say_hello())  # Output: HELLO\n",
      "```\n",
      "\n",
      "Let's break down this example:\n",
      "\n",
      "1. We define a decorator function `uppercase_decorator` that takes a function `func` as an argument.\n",
      "2. Inside `uppercase_decorator`, we define a nested function `wrapper` that calls the original function `func` and then converts the result to uppercase using the `upper()` method.\n",
      "3. The `wrapper` function is returned by `uppercase_decorator`.\n",
      "4. We define a simple function `say_hello` that returns the string \"hello\".\n",
      "5. We decorate the `say_hello` function by assigning it to the result of `uppercase_decorator(say_hello)`. This means that `say_hello` is now wrapped by the `wrapper` function returned by `uppercase_decorator`.\n",
      "6. When we call `say_hello()`, it actually calls the `wrapper` function, which in turn calls the original `say_hello` function and converts the result to uppercase.\n",
      "\n",
      "In this example, the `uppercase_decorator` adds the functionality of converting the output of the `say_hello` function to uppercase without modifying the `say_hello` function itself.\n",
      "\n",
      "Decorators can be used for various purposes, such as logging, caching, authentication, and more. They provide a way to add functionality to existing functions in a clean and modular way, promoting code reusability and separation of concerns.\n"
     ]
    }
   ],
   "source": [
    "# Test LangChain Prompt Templates\n",
    "try:\n",
    "    from langchain_core.prompts import ChatPromptTemplate\n",
    "    from langchain_core.output_parsers import StrOutputParser\n",
    "    print(\"✓ Using new prompt imports\")\n",
    "except ImportError:\n",
    "    from langchain.prompts import ChatPromptTemplate\n",
    "    from langchain.schema import StrOutputParser\n",
    "    print(\"✓ Using legacy prompt imports\")\n",
    "\n",
    "# Create a prompt template\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful coding tutor. Explain concepts clearly with examples.\"),\n",
    "    (\"human\", \"Explain {concept} in Python with a simple example.\")\n",
    "])\n",
    "\n",
    "# Create a chain\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "# Test the chain\n",
    "result = chain.invoke({\"concept\": \"decorators\"})\n",
    "print(\"Chain result - Python Decorators:\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "16bf17a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Streaming response from Claude:\n",
      "--------------------------------------------------\n",
      "Here's a short poem about artificial intelligence:\n",
      "\n",
      "Artificial Intelligence, a marvel of our age,\n",
      "Circuits and codes, a digital sage.\n",
      "Algorithms weave, data's tapestry unfurled,\n",
      "Unveiling insights, shaping our world.\n",
      "\n",
      "Machines that learn, evolving with each task,\n",
      "Patterns discerned, solutions they unmask.\n",
      "From chatbots' wit to self-driving cars,\n",
      "AI's potential reaches to the stars.\n",
      "\n",
      "Yet, as we embrace this technological might,\n",
      "Ethics must guide us, lest we lose sight.\n",
      "For in these bytes, a power profound,\n",
      "Humanity's values must be firmly bound.\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Test streaming functionality\n",
    "print(\"Streaming response from Claude:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for chunk in llm.stream(\"Write a short poem about artificial intelligence\"):\n",
    "    print(chunk.content, end=\"\", flush=True)\n",
    "\n",
    "print(\"\\n\" + \"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "747c8871",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing different Claude models:\n",
      "============================================================\n",
      "\n",
      "claude-3-haiku-20240307:\n",
      "Response: The capital of France is Paris.\n",
      "\n",
      "claude-3-sonnet-20240229:\n",
      "Response: The capital of France is Paris.\n",
      "\n",
      "claude-3-opus-20240229:\n",
      "Response: The capital of France is Paris.\n"
     ]
    }
   ],
   "source": [
    "# Test switching between different Claude models\n",
    "test_question = \"What is the capital of France?\"\n",
    "models_to_test = [\n",
    "    \"claude-3-haiku-20240307\",    # Fastest, cheapest\n",
    "    \"claude-3-sonnet-20240229\",   # Balanced\n",
    "    \"claude-3-opus-20240229\"      # Most capable (if you have access)\n",
    "]\n",
    "\n",
    "print(\"Testing different Claude models:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for model in models_to_test:\n",
    "    try:\n",
    "        llm.switch_model(llm=model, provider=\"claude\")\n",
    "        response = llm.invoke(test_question)\n",
    "        print(f\"\\n{model}:\")\n",
    "        print(f\"Response: {response.content}\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n{model}: Error - {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e03bd553",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Memory imports successful\n",
      "Conversation with memory:\n",
      "------------------------------\n",
      "User: My name is Alice and I love Python programming.\n",
      "Claude: It's great to meet you, Alice! It's wonderful that you have a passion for Python programming. Python is a versatile and beginner-friendly language that is widely used for various applications, including web development, data analysis, machine learning, and more.\n",
      "\n",
      "As an AI language model, I can assist you with any questions or challenges you may encounter while learning or working with Python. Feel free to ask me about Python concepts, syntax, libraries, or any specific problems you're trying to solve.\n",
      "\n",
      "To get started, could you tell me a bit more about your experience with Python so far? Are you just beginning your Python journey, or do you have some prior knowledge of the language? Knowing your current level will help me provide you with the most relevant information and guidance.\n",
      "\n",
      "User: What's my name and what do I love?\n",
      "Claude: Your name is Alice and you love Python programming.\n"
     ]
    }
   ],
   "source": [
    "#Advanced LangChain Features - Memory\n",
    "\n",
    "try:\n",
    "    from langchain.memory import ConversationBufferMemory\n",
    "    from langchain_core.messages import HumanMessage, AIMessage\n",
    "    print(\"✓ Memory imports successful\")\n",
    "except ImportError:\n",
    "    from langchain.schema import HumanMessage, AIMessage\n",
    "    print(\"✓ Using basic message imports\")\n",
    "    # Memory might not be available, so we'll implement simple history\n",
    "\n",
    "# Create a simple conversation memory\n",
    "conversation_history = []\n",
    "\n",
    "def chat_with_memory(user_input):\n",
    "    \"\"\"Simple chat function with memory\"\"\"\n",
    "    # Add user message\n",
    "    conversation_history.append(HumanMessage(content=user_input))\n",
    "    \n",
    "    # Get response from Claude\n",
    "    response = llm.invoke(conversation_history)\n",
    "    \n",
    "    # Add AI response to history\n",
    "    conversation_history.append(response)\n",
    "    \n",
    "    return response.content\n",
    "\n",
    "# Test conversation with memory\n",
    "print(\"Conversation with memory:\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "response1 = chat_with_memory(\"My name is Alice and I love Python programming.\")\n",
    "print(f\"User: My name is Alice and I love Python programming.\")\n",
    "print(f\"Claude: {response1}\")\n",
    "print()\n",
    "\n",
    "response2 = chat_with_memory(\"What's my name and what do I love?\")\n",
    "print(f\"User: What's my name and what do I love?\")\n",
    "print(f\"Claude: {response2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "20274d03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ New tool imports successful\n",
      "Testing tool usage with Claude:\n",
      "Calculator tool result: The result is: 110\n",
      "Claude's verification: I'm sorry, but the result you provided is not correct. Let's solve this problem step by step to find the correct answer.\n",
      "\n",
      "Given:\n",
      "- The problem is to calculate 25 * 4 + 10\n",
      "\n",
      "Step 1: Perform the multiplication.\n",
      "25 * 4 = 100\n",
      "\n",
      "Step 2: Add 10 to the result of the multiplication.\n",
      "100 + 10 = 110\n",
      "\n",
      "Therefore, the correct result of the expression 25 * 4 + 10 is 110.\n",
      "\n",
      "Your calculation was correct, but it seems like there was a misunderstanding in your statement. The result you provided (110) is indeed the correct answer to the given problem.\n"
     ]
    }
   ],
   "source": [
    "# Test LangChain Tools (Simple Example)\n",
    "\n",
    "# import libraries\n",
    "try:\n",
    "    # Try new LangChain structure first\n",
    "    from langchain_core.tools import BaseTool\n",
    "    from pydantic import BaseModel, Field\n",
    "    print(\"✓ New tool imports successful\")\n",
    "    TOOLS_AVAILABLE = True\n",
    "except ImportError:\n",
    "    try:\n",
    "        # Try legacy structure\n",
    "        from langchain.tools import BaseTool\n",
    "        from pydantic import BaseModel, Field\n",
    "        print(\"✓ Legacy tool imports successful\")\n",
    "        TOOLS_AVAILABLE = True\n",
    "    except ImportError:\n",
    "        print(\"⚠️ Tool imports not available - using function-based approach\")\n",
    "        BaseTool = None\n",
    "        BaseModel = None\n",
    "        Field = None\n",
    "        TOOLS_AVAILABLE = False\n",
    "\n",
    "# Create a simple custom tool\n",
    "if BaseTool and BaseModel:\n",
    "    class CalculatorInput(BaseModel):\n",
    "        expression: str = Field(description=\"Mathematical expression to evaluate\")\n",
    "\n",
    "    class CalculatorTool(BaseTool):\n",
    "        # Properly annotate all fields for Pydantic v2 compatibility\n",
    "        name: str = \"calculator\"\n",
    "        description: str = \"Useful for mathematical calculations\"\n",
    "        args_schema: type[BaseModel] = CalculatorInput\n",
    "        \n",
    "        def _run(self, expression: str) -> str:\n",
    "            try:\n",
    "                # Use a safer eval alternative for production\n",
    "                import ast\n",
    "                import operator\n",
    "                \n",
    "                # Simple math operations mapping\n",
    "                ops = {\n",
    "                    ast.Add: operator.add,\n",
    "                    ast.Sub: operator.sub,\n",
    "                    ast.Mult: operator.mul,\n",
    "                    ast.Div: operator.truediv,\n",
    "                    ast.Pow: operator.pow,\n",
    "                    ast.USub: operator.neg,\n",
    "                }\n",
    "                \n",
    "                def safe_eval(node):\n",
    "                    if isinstance(node, ast.Constant):  # Numbers\n",
    "                        return node.value\n",
    "                    elif isinstance(node, ast.BinOp):  # Binary operations\n",
    "                        return ops[type(node.op)](safe_eval(node.left), safe_eval(node.right))\n",
    "                    elif isinstance(node, ast.UnaryOp):  # Unary operations\n",
    "                        return ops[type(node.op)](safe_eval(node.operand))\n",
    "                    else:\n",
    "                        raise TypeError(f\"Unsupported operation: {type(node)}\")\n",
    "                \n",
    "                try:\n",
    "                    # Parse and evaluate safely\n",
    "                    tree = ast.parse(expression, mode='eval')\n",
    "                    result = safe_eval(tree.body)\n",
    "                    return f\"The result is: {result}\"\n",
    "                except:\n",
    "                    # Fallback to basic eval for simple expressions (be careful in production!)\n",
    "                    result = eval(expression)\n",
    "                    return f\"The result is: {result}\"\n",
    "                    \n",
    "            except Exception as e:\n",
    "                return f\"Error calculating: {e}\"\n",
    "\n",
    "    # Test the tool\n",
    "    try:\n",
    "        print(\"Testing tool usage with Claude:\")\n",
    "        calculator = CalculatorTool()\n",
    "        calc_result = calculator._run(\"25 * 4 + 10\")\n",
    "        print(f\"Calculator tool result: {calc_result}\")\n",
    "\n",
    "        # Ask Claude to use the result\n",
    "        response = llm.invoke(f\"I calculated 25 * 4 + 10 and got: {calc_result}. Can you verify this is correct?\")\n",
    "        print(f\"Claude's verification: {response.content}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Tool creation failed with error: {e}\")\n",
    "        print(\"This is likely due to LangChain version compatibility issues.\")\n",
    "        print(\"Let's try a simpler approach...\")\n",
    "        \n",
    "        # Simple function-based approach without BaseTool\n",
    "        def simple_calculator(expression: str) -> str:\n",
    "            try:\n",
    "                result = eval(expression)  # Note: Use a safer parser in production\n",
    "                return f\"The result is: {result}\"\n",
    "            except Exception as e:\n",
    "                return f\"Error calculating: {e}\"\n",
    "        \n",
    "        print(\"Using simple function approach:\")\n",
    "        calc_result = simple_calculator(\"25 * 4 + 10\")\n",
    "        print(f\"Calculator result: {calc_result}\")\n",
    "        \n",
    "        response = llm.invoke(f\"I calculated 25 * 4 + 10 and got: {calc_result}. Can you verify this is correct?\")\n",
    "        print(f\"Claude's verification: {response.content}\")\n",
    "\n",
    "else:\n",
    "    print(\"Skipping tool example due to import issues\")\n",
    "    print(\"This is normal in newer LangChain versions where agent structure has changed\")\n",
    "    \n",
    "    # Alternative: Simple function-based tool demonstration\n",
    "    print(\"\\nUsing simple function approach instead:\")\n",
    "    \n",
    "    def calculator_function(expression: str) -> str:\n",
    "        \"\"\"Simple calculator function\"\"\"\n",
    "        try:\n",
    "            result = eval(expression)  # Note: Use a safer math parser in production\n",
    "            return f\"The result is: {result}\"\n",
    "        except Exception as e:\n",
    "            return f\"Error calculating: {e}\"\n",
    "    \n",
    "    # Test the function\n",
    "    calc_result = calculator_function(\"25 * 4 + 10\")\n",
    "    print(f\"Calculator function result: {calc_result}\")\n",
    "    \n",
    "    # Ask Claude about it\n",
    "    response = llm.invoke(f\"I used a calculator function and got: {calc_result}. Is this calculation correct?\")\n",
    "    print(f\"Claude's response: {response.content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ea403d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Switch to Different Providers (if available)\n",
    "\n",
    "# %%\n",
    "# Test switching to OpenAI if available\n",
    "if os.getenv(\"OPENAI_API_KEY\"):\n",
    "    print(\"Testing provider switching:\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    # Claude response\n",
    "    llm.switch_model(llm=\"claude-3-sonnet-20240229\", provider=\"claude\")\n",
    "    claude_response = llm.invoke(\"What's the capital of France?\")\n",
    "    print(f\"Claude: {claude_response.content}\")\n",
    "    \n",
    "    # Switch to OpenAI\n",
    "    llm.switch_model(llm=\"gpt-3.5-turbo\", provider=\"openai\")\n",
    "    openai_response = llm.invoke(\"What's the capital of France?\")\n",
    "    print(f\"OpenAI: {openai_response.content}\")\n",
    "    \n",
    "    # Switch back to Claude\n",
    "    llm.switch_model(llm=\"claude-3-sonnet-20240229\", provider=\"claude\")\n",
    "    print(f\"Switched back to: {llm.current_model_info}\")\n",
    "else:\n",
    "    print(\"OpenAI API key not available - skipping provider switching test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "544fcac8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creative Response:\n",
      "Here's a poem about AI:\n",
      "\n",
      "Artificial Intelligence, a marvel of our time,\n",
      "A creation of code, a symphony sublime.\n",
      "Algorithms dance with data's vast embrace,\n",
      "Unraveling patterns, leaving no trace.\n",
      "\n",
      "Neural networks intertwine, a labyrinth of thought,\n",
      "Processing knowledge that can't be bought.\n",
      "Machine learning's evolution, a ceaseless quest,\n",
      "Adapting, growing, forever blessed.\n",
      "\n",
      "From language processing to image recognition,\n",
      "AI's prowess knows no restriction.\n",
      "Automating tasks, enhancing human life,\n",
      "Cutting through complexity's knife.\n",
      "\n",
      "Yet, amidst its brilliance, a lingering fear,\n",
      "Of ethical boundaries we must hold dear.\n",
      "A delicate balance, a dance we must lead,\n",
      "Harnessing AI's power, while planting wisdom's seed.\n",
      "\n",
      "Embrace the future, where AI reigns,\n",
      "But let humanity's values guide the reins.\n",
      "For in this symbiosis, our true strength lies,\n",
      "Merging technology and wisdom's wise eyes.\n"
     ]
    }
   ],
   "source": [
    "from llm_controller import AdaptiveLLM\n",
    "\n",
    "adaptive = AdaptiveLLM()\n",
    "\n",
    "creative_response = adaptive.query(\"Write a poem about AI\", \"creative\")\n",
    "print(\"Creative Response:\")\n",
    "print(creative_response.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "35494b1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Claude's Code Review:\n",
      "==================================================\n",
      "The provided Python code calculates the average of a list of numbers. Here's my feedback based on the requested areas:\n",
      "\n",
      "1. **Code Quality**:\n",
      "   - The code is readable and follows a straightforward approach.\n",
      "   - The function name `calculate_average` is descriptive and accurately represents its purpose.\n",
      "   - The code uses appropriate variable names (`total` and `i`).\n",
      "\n",
      "2. **Best Practices**:\n",
      "   - The code follows the Python style guide (PEP 8) for naming conventions and code formatting.\n",
      "   - The use of a loop to iterate over the list and calculate the sum is a common and acceptable practice.\n",
      "   - The code uses the built-in `len()` function to determine the length of the list, which is a good practice.\n",
      "\n",
      "3. **Potential Improvements**:\n",
      "   - Instead of using a traditional `for` loop with `range(len(numbers))`, you could use the more Pythonic way of iterating over the list directly:\n",
      "\n",
      "     ```python\n",
      "     def calculate_average(numbers):\n",
      "         total = sum(numbers)\n",
      "         return total / len(numbers)\n",
      "     ```\n",
      "\n",
      "     This approach uses the built-in `sum()` function to calculate the total, making the code more concise and readable.\n",
      "\n",
      "   - You could also consider adding input validation to handle empty lists or lists containing non-numeric values. For example:\n",
      "\n",
      "     ```python\n",
      "     def calculate_average(numbers):\n",
      "         if not numbers:\n",
      "             return 0  # or raise an exception, depending on your requirements\n",
      "         \n",
      "         total = sum(numbers)\n",
      "         return total / len(numbers)\n",
      "     ```\n",
      "\n",
      "   - If you want to handle non-numeric values in the list, you could use a list comprehension or a generator expression to filter out non-numeric values before calculating the average:\n",
      "\n",
      "     ```python\n",
      "     def calculate_average(numbers):\n",
      "         valid_numbers = [num for num in numbers if isinstance(num, (int, float))]\n",
      "         if not valid_numbers:\n",
      "             return 0  # or raise an exception, depending on your requirements\n",
      "         \n",
      "         total = sum(valid_numbers)\n",
      "         return total / len(valid_numbers)\n",
      "     ```\n",
      "\n",
      "4. **Bugs or Issues**:\n",
      "   - The provided code does not handle edge cases such as an empty list or a list containing non-numeric values. If the list is empty, the code will raise a `ZeroDivisionError` when attempting to divide by the length of the list.\n",
      "\n",
      "Overall, the code is simple and functional, but there are opportunities for improvement in terms of handling edge cases, using more Pythonic constructs, and potentially adding input validation.\n"
     ]
    }
   ],
   "source": [
    "# Final Test - Complex Chain\n",
    "\n",
    "llm = LLMController(\n",
    "    llm=\"claude-3-sonnet-20240229\",  # You can change this to claude-3-haiku-20240307 for faster/cheaper\n",
    "    provider=\"claude\"\n",
    ")\n",
    "\n",
    "try:\n",
    "    from langchain_core.prompts import ChatPromptTemplate\n",
    "    from langchain_core.output_parsers import StrOutputParser\n",
    "except ImportError:\n",
    "    from langchain.prompts import ChatPromptTemplate\n",
    "    from langchain.schema import StrOutputParser\n",
    "\n",
    "# Create a complex chain for code review\n",
    "code_review_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are an expert Python code reviewer. Provide constructive feedback.\"),\n",
    "    (\"human\", \"\"\"\n",
    "    Please review this Python code and provide feedback:\n",
    "    \n",
    "    ```python\n",
    "    {code}\n",
    "    ```\n",
    "    \n",
    "    Focus on:\n",
    "    1. Code quality\n",
    "    2. Best practices\n",
    "    3. Potential improvements\n",
    "    4. Any bugs or issues\n",
    "    \"\"\")\n",
    "])\n",
    "\n",
    "# Sample code to review\n",
    "sample_code = \"\"\"\n",
    "def calculate_average(numbers):\n",
    "    total = 0\n",
    "    for i in range(len(numbers)):\n",
    "        total = total + numbers[i]\n",
    "    return total / len(numbers)\n",
    "\n",
    "result = calculate_average([1, 2, 3, 4, 5])\n",
    "print(result)\n",
    "\"\"\"\n",
    "\n",
    "# Create and run the chain\n",
    "review_chain = code_review_prompt | llm | StrOutputParser()\n",
    "review_result = review_chain.invoke({\"code\": sample_code})\n",
    "\n",
    "print(\"Claude's Code Review:\")\n",
    "print(\"=\" * 50)\n",
    "print(review_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c746821",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
